"""
è‚¡ç¥¨çŸ­çº¿äº¤æ˜“é¢„æµ‹æ¨¡å‹
æ”¯æŒå¤šç§æœºå™¨å­¦ä¹ ç®—æ³•ï¼šLSTMã€XGBoostã€éšæœºæ£®æ—ã€SVM
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.svm import SVR, SVC
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error
import xgboost as xgb
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class StockDataset(Dataset):
    """PyTorchæ•°æ®é›†ç±»"""
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

class SimpleLSTM(nn.Module):
    """ç®€å•çš„LSTMæ¨¡å‹"""
    def __init__(self, input_size, hidden_size=50, num_layers=2, output_size=1, dropout=0.2):
        super(SimpleLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # LSTMå±‚
        lstm_out, _ = self.lstm(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        lstm_out = lstm_out[:, -1, :]
        # Dropout
        lstm_out = self.dropout(lstm_out)
        # å…¨è¿æ¥å±‚
        output = self.fc(lstm_out)
        return output

class StockPredictionModels:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.results = {}
        
    def prepare_data(self, data, target_col='close', lookback_days=5):
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        """
        # åˆ›å»ºæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        data = data.copy()
        
        # ç§»åŠ¨å¹³å‡çº¿
        data['ma5'] = data[target_col].rolling(window=5).mean()
        data['ma10'] = data[target_col].rolling(window=10).mean()
        data['ma20'] = data[target_col].rolling(window=20).mean()
        
        # ä»·æ ¼å˜åŒ–ç‡
        data['price_change'] = data[target_col].pct_change()
        data['price_change_5'] = data[target_col].pct_change(5)
        
        # æ³¢åŠ¨ç‡
        data['volatility'] = data['price_change'].rolling(window=5).std()
        
        # RSIæŒ‡æ ‡
        delta = data[target_col].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        data['rsi'] = 100 - (100 / (1 + rs))
        
        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        data = data.dropna()
        
        # åˆ›å»ºç‰¹å¾å’Œç›®æ ‡å˜é‡
        feature_cols = ['ma5', 'ma10', 'ma20', 'price_change', 'price_change_5', 'volatility', 'rsi']
        X = data[feature_cols].values
        
        # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆä¸‹ä¸€å¤©çš„ä»·æ ¼å˜åŒ–ï¼‰
        y_price = data[target_col].shift(-1).dropna().values
        X = X[:-1]  # å¯¹åº”è°ƒæ•´Xçš„é•¿åº¦
        
        # åˆ›å»ºåˆ†ç±»ç›®æ ‡ï¼ˆæ¶¨è·Œï¼‰
        y_class = (y_price > data[target_col].iloc[:-1].values).astype(int)
        
        return X, y_price, y_class
    
    def create_lstm_data(self, data, target_col='close', lookback_days=10):
        """
        ä¸ºLSTMåˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
        """
        # å‡†å¤‡åŸºç¡€æ•°æ®
        X, y_price, y_class = self.prepare_data(data, target_col)
        
        # åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
        X_lstm, y_lstm = [], []
        for i in range(lookback_days, len(X)):
            X_lstm.append(X[i-lookback_days:i])
            y_lstm.append(y_price[i])
        
        return np.array(X_lstm), np.array(y_lstm)
    
    def build_lstm_model(self, input_shape):
        """
        æ„å»ºPyTorch LSTMæ¨¡å‹
        """
        model = SimpleLSTM(
            input_size=input_shape[1],  # ç‰¹å¾æ•°é‡
            hidden_size=50,
            num_layers=2,
            output_size=1,
            dropout=0.2
        )
        return model.to(device)
    
    def train_lstm(self, data, target_col='close', lookback_days=10):
        """
        è®­ç»ƒLSTMæ¨¡å‹
        """
        print("è®­ç»ƒLSTMæ¨¡å‹...")
        
        # å‡†å¤‡æ•°æ®
        X, y = self.create_lstm_data(data, target_col, lookback_days)
        
        # æ•°æ®æ ‡å‡†åŒ–
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        
        X_scaled = scaler_X.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
        y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_scaled, test_size=0.2, random_state=42
        )
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        train_dataset = StockDataset(X_train, y_train)
        test_dataset = StockDataset(X_test, y_test)
        
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        # æ„å»ºæ¨¡å‹
        model = self.build_lstm_model(X.shape)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        # è®­ç»ƒæ¨¡å‹
        model.train()
        train_losses = []
        
        for epoch in range(50):
            epoch_loss = 0
            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                
                optimizer.zero_grad()
                outputs = model(batch_X).squeeze()
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
            
            avg_loss = epoch_loss / len(train_loader)
            train_losses.append(avg_loss)
            
            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {avg_loss:.6f}")
        
        # è¯„ä¼°æ¨¡å‹
        model.eval()
        predictions = []
        actuals = []
        
        with torch.no_grad():
            for batch_X, batch_y in test_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X).squeeze()
                predictions.extend(outputs.cpu().numpy())
                actuals.extend(batch_y.cpu().numpy())
        
        predictions = np.array(predictions)
        actuals = np.array(actuals)
        
        # åæ ‡å‡†åŒ–
        predictions = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()
        actuals = scaler_y.inverse_transform(actuals.reshape(-1, 1)).flatten()
        
        # è®¡ç®—æŒ‡æ ‡
        mse = mean_squared_error(actuals, predictions)
        mae = mean_absolute_error(actuals, predictions)
        
        # ä¿å­˜æ¨¡å‹å’Œç¼©æ”¾å™¨
        self.models['lstm'] = model
        self.scalers['lstm'] = {'X': scaler_X, 'y': scaler_y}
        
        return {
            'model': 'LSTM',
            'mse': mse,
            'mae': mae,
            'rmse': np.sqrt(mse),
            'train_losses': train_losses
        }
    
    def train_xgboost(self, data, target_type='classification'):
        """
        è®­ç»ƒXGBoostæ¨¡å‹
        """
        print(f"è®­ç»ƒXGBoostæ¨¡å‹ ({target_type})...")
        
        X, y_price, y_class = self.prepare_data(data)
        
        if target_type == 'classification':
            y = y_class
            model = xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            )
        else:
            y = y_price
            model = xgb.XGBRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            )
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        
        # ä¿å­˜æ¨¡å‹
        self.models[f'xgboost_{target_type}'] = model
        
        if target_type == 'classification':
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted')
            recall = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')
            
            return {
                'model': f'XGBoost ({target_type})',
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1
            }
        else:
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            
            return {
                'model': f'XGBoost ({target_type})',
                'mse': mse,
                'mae': mae,
                'rmse': np.sqrt(mse)
            }
    
    def train_random_forest(self, data, target_type='classification'):
        """
        è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
        """
        print(f"è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ ({target_type})...")
        
        X, y_price, y_class = self.prepare_data(data)
        
        if target_type == 'classification':
            y = y_class
            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
        else:
            y = y_price
            model = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        
        # ä¿å­˜æ¨¡å‹
        self.models[f'rf_{target_type}'] = model
        
        if target_type == 'classification':
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted')
            recall = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')
            
            return {
                'model': f'Random Forest ({target_type})',
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1
            }
        else:
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            
            return {
                'model': f'Random Forest ({target_type})',
                'mse': mse,
                'mae': mae,
                'rmse': np.sqrt(mse)
            }
    
    def train_svm(self, data, target_type='classification'):
        """
        è®­ç»ƒSVMæ¨¡å‹
        """
        print(f"è®­ç»ƒSVMæ¨¡å‹ ({target_type})...")
        
        X, y_price, y_class = self.prepare_data(data)
        
        # æ•°æ®æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        if target_type == 'classification':
            y = y_class
            model = SVC(kernel='rbf', C=1.0, random_state=42)
        else:
            y = y_price
            model = SVR(kernel='rbf', C=1.0)
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        
        # ä¿å­˜æ¨¡å‹å’Œç¼©æ”¾å™¨
        self.models[f'svm_{target_type}'] = model
        self.scalers[f'svm_{target_type}'] = scaler
        
        if target_type == 'classification':
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted')
            recall = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')
            
            return {
                'model': f'SVM ({target_type})',
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1
            }
        else:
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            
            return {
                'model': f'SVM ({target_type})',
                'mse': mse,
                'mae': mae,
                'rmse': np.sqrt(mse)
            }
    
    def compare_models(self, data):
        """
        æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½
        """
        print("å¼€å§‹æ¨¡å‹æ¯”è¾ƒ...")
        
        # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        results = []
        
        # LSTM
        try:
            lstm_result = self.train_lstm(data)
            results.append(lstm_result)
            self.results['lstm'] = lstm_result
        except Exception as e:
            print(f"LSTMè®­ç»ƒå¤±è´¥: {e}")
        
        # XGBoost
        try:
            xgb_class_result = self.train_xgboost(data, 'classification')
            xgb_reg_result = self.train_xgboost(data, 'regression')
            results.extend([xgb_class_result, xgb_reg_result])
            self.results['xgboost_classification'] = xgb_class_result
            self.results['xgboost_regression'] = xgb_reg_result
        except Exception as e:
            print(f"XGBoostè®­ç»ƒå¤±è´¥: {e}")
        
        # Random Forest
        try:
            rf_class_result = self.train_random_forest(data, 'classification')
            rf_reg_result = self.train_random_forest(data, 'regression')
            results.extend([rf_class_result, rf_reg_result])
            self.results['rf_classification'] = rf_class_result
            self.results['rf_regression'] = rf_reg_result
        except Exception as e:
            print(f"Random Forestè®­ç»ƒå¤±è´¥: {e}")
        
        # SVM
        try:
            svm_class_result = self.train_svm(data, 'classification')
            svm_reg_result = self.train_svm(data, 'regression')
            results.extend([svm_class_result, svm_reg_result])
            self.results['svm_classification'] = svm_class_result
            self.results['svm_regression'] = svm_reg_result
        except Exception as e:
            print(f"SVMè®­ç»ƒå¤±è´¥: {e}")
        
        return results
    
    def print_results(self):
        """
        æ‰“å°æ‰€æœ‰æ¨¡å‹çš„ç»“æœ
        """
        print("\n" + "="*60)
        print("æ¨¡å‹æ€§èƒ½æ¯”è¾ƒç»“æœ")
        print("="*60)
        
        for model_name, result in self.results.items():
            print(f"\n{result['model']}:")
            for metric, value in result.items():
                if metric != 'model' and metric != 'train_losses':
                    if isinstance(value, float):
                        print(f"  {metric}: {value:.4f}")
                    else:
                        print(f"  {metric}: {value}")
    
    def get_recommendation(self):
        """
        åŸºäºæ¨¡å‹ç»“æœç»™å‡ºæ¨è
        """
        print("\n" + "="*60)
        print("æ¨¡å‹æ¨è")
        print("="*60)
        
        print("\nğŸ“Š åˆ†ç±»ä»»åŠ¡æ¨è (é¢„æµ‹æ¶¨è·Œ):")
        print("1. XGBoost: åœ¨é‡‘èæ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹å¾é‡è¦æ€§æ¸…æ™°")
        print("2. Random Forest: ç¨³å®šæ€§å¥½ï¼ŒæŠ—è¿‡æ‹Ÿåˆ")
        print("3. SVM: åœ¨å°æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½")
        
        print("\nğŸ“ˆ å›å½’ä»»åŠ¡æ¨è (é¢„æµ‹ä»·æ ¼):")
        print("1. LSTM: æ“…é•¿æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œèƒ½æ•æ‰é•¿æœŸä¾èµ–")
        print("2. XGBoost: éçº¿æ€§å…³ç³»å¤„ç†èƒ½åŠ›å¼º")
        print("3. Random Forest: ç¨³å®šå¯é çš„åŸºå‡†æ¨¡å‹")
        
        print("\nğŸ’¡ å®é™…åº”ç”¨å»ºè®®:")
        print("- çŸ­çº¿äº¤æ˜“: æ¨èXGBooståˆ†ç±»æ¨¡å‹")
        print("- ä»·æ ¼é¢„æµ‹: æ¨èLSTMå›å½’æ¨¡å‹")
        print("- é£é™©æ§åˆ¶: æ¨èRandom Forestï¼ˆç¨³å®šæ€§å¥½ï¼‰")
        print("- æ¨¡å‹é›†æˆ: ç»“åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ")

if __name__ == "__main__":
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    print("è‚¡ç¥¨é¢„æµ‹æ¨¡å‹æµ‹è¯•")
    print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€éœ€ä¾èµ–ï¼špip install torch xgboost scikit-learn")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®
    import pandas as pd
    from datetime import datetime, timedelta
    
    dates = pd.date_range('2023-01-01', periods=500, freq='D')
    np.random.seed(42)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®
    price = 100
    prices = [price]
    for i in range(499):
        change = np.random.normal(0, 0.02)
        price = price * (1 + change)
        prices.append(price)
    
    data = pd.DataFrame({
        'date': dates,
        'close': prices
    })
    
    # åˆ›å»ºæ¨¡å‹æ¯”è¾ƒå™¨
    models = StockPredictionModels()
    
    # æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹
    results = models.compare_models(data)
    
    # æ‰“å°ç»“æœ
    models.print_results()
    
    # ç»™å‡ºæ¨è
    models.get_recommendation()